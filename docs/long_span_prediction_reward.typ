#import "_template.typ": *

// このファイルは単独でコンパイルできる独立したドキュメント
#show: simple-document.with(
  title: "長期予測誤差を用いた好奇心ベースの学習",
  author: "GesonAnko",
  date: "2025-02-28",
)

= 長期予測誤差を用いた好奇心ベースの学習

== 1. 基本的な好奇心の目的関数
好奇心ベースのエージェントでは、世界モデル（予測器）の予測誤差を最大化するように行動を選択することで、未知の状態や興味深い結果を探索します。このメカニズムを定式化すると、以下のようになります。

状態 $s_t$、行動 $a_t$、次の状態 $s_(t+1)$ について考えます。パラメータ $theta$ を持つ予測器を $f_theta$ とし、その出力する次の状態の分布を $p^f (dot.op|s_t, a_t)$ とします: 

$ p^f arrow.l f_theta (s_t, a_t) $
$ I_(t+1) = -log p^f (s_(t+1)|s_t, a_t) $

ここで $I_(t+1)$ は予測誤差（自己情報量または「驚き」）を表します。

一方、パラメータ $phi$ を持つ行動生成器（方策）を $pi_phi$ とし、その出力する行動の分布を $p^pi (dot.op|s_t)$ とすると: 

$ p^pi (dot.op|s_t) arrow.l pi_phi (s_t) $
$ a_t tilde p^pi (dot.op|s_t) $

となります。

このとき、予測器と行動生成器の学習目標は以下のように定義されます: 

$ phi arrow.l arg max_phi I $
$ theta arrow.l arg min_theta I $

この「ゼロサム的な敵対的学習関係」が好奇心ベースのエージェントの核心です。予測器は誤差を最小化しようとし、行動生成器はその予測を困難にするような状態を生み出そうとします。

== 2. 時系列データへの拡張
実世界では、システムの完全な状態 $s$ を直接観測できないことが多く、代わりに観測 $o_t$ のみが得られます。そこで、予測器 $f_theta$ に隠れ状態 $h^f_t$ を持たせ、時間的文脈を扱えるようにします: 

$ p^f, h^(f)_t arrow.l f_theta (o_t, a_t, h^(f)_(t-1)) $
$ I = -log p^f (o_(t+1)|o_t, a_t, h^(f)_(t-1)) $

同様に、方策 $pi_phi$ も隠れ状態 $h^(pi)_t$ を持つように拡張します: 

$ p^pi, h^(pi)_t arrow.l pi_phi (o_t, h^(pi)_(t-1)) $

== 3. 長期予測の重要性と課題
単純な形式では、方策 $pi_phi$ は1ステップ先の予測誤差のみを最大化するよう学習します。しかし、これでは「退屈なトンネルを抜けた先にある素晴らしい景色」のような、短期的には予測可能だが長期的には予測困難な状況を探索するインセンティブがありません。

長期的な視点を取り入れるには、複数ステップ先の予測誤差を考慮する必要があります。ただし、これは単純ではありません。予測器の損失関数と方策の目的関数の両方を適切に調整し、敵対的学習関係を維持する必要があります。

== 4. 長期予測誤差を考慮した目的関数
長期予測を組み込むため、固定ホライズン $D$ を考え、時刻 $t$ から $d$ ステップ先の観測を予測する誤差を定義します: 

$ I_(t,d) = -log p^f (o_(t+d+1) | o_t, a_(t:t+d), h^(f)_(t-1)) $

ここで、$d in {0, 1, ..., D-1}$ は予測ステップを表し、$a_(t:t+d)$ は時刻 $t$ から $t+d$ までの行動列を示します。

各時刻 $t$ における「マルチステップ予測誤差」は、ホライズン $D$ までの予測誤差の加重平均として定義できます: 

$ I_t^(text("multi")) = sum_(d=0)^(D-1) w_d dot I_(t,d) $

ここで $w_d$ は各ステップの重みです。

== 5. 勾配安定性の問題と解決策
単純に全ステップを均等に重み付けすると（$w_d = 1/D$）、遠い将来の予測ほど誤差が累積し、勾配が不安定になる問題が生じます。この問題を解決するため、指数的な減衰を導入します: 

$ w_d = (gamma^d)/(sum_(i=0)^(D-1) gamma^i) = ((1-gamma)gamma^d)/(1-gamma^D) $

ここで $gamma in (0,1]$ は減衰係数です。$gamma = 1$ の場合は均等な重み付けとなり、$gamma < 1$ の場合は遠い将来ほど小さい重みが与えられます。

ホライズン $D$ に応じた自然な設定として、次のような $gamma$ が提案されます: 

$ gamma = e^(-1/D) $

この設定により、$D$ が大きいほど $gamma$ は1に近づき、遠い将来への減衰が緩やかになります。最終ホライズン $d=D-1$ での重みは約 $e^(-1) approx 0.368$ となります。

== 6. 最終的な目的関数
以上を踏まえ、世界予測器（予測モデル）$f_theta$ の最終的な損失関数は以下のように定式化されます: 

#rect[
$ cal(L)_f = lim_(T arrow infinity) 1/T sum_(t=0)^(T-1) (1-gamma)/(1-gamma^D) sum_(d=0)^(D-1) gamma^d [-log p^f (o_(t+d+1) | o_t, a_(t:t+d), h^(f)_(t-1))] $
]

ここで $gamma = e^(- 1/D)$ です。

一方、行動生成器（方策）$pi_phi$ の目的関数は、この長期予測誤差を最大化するように設定されます: 

#rect[
$ J(phi) = lim_(T arrow infinity) 1/T sum_(t=0)^(T-1) E_(pi_phi) [(1-gamma)/(1-gamma^D) sum_(d=0)^(D-1) gamma^d [-log p^f (o_(t+d+1) | o_t, a_(t:t+d), h^(f)_(t-1))]] $
]

== 7. 理論的特性と実装上の考慮点
この定式化には以下の特性があります: 

1. *敵対的学習関係の維持*: 世界モデルが最小化する目的関数と方策が最大化する目的関数が一致しており、ゼロサム的な関係が保たれます。

2. *長期予測の考慮*: 単なる1ステップ先だけでなく、複数ステップ先の予測も考慮し、長期的な探索を促進します。

3. *勾配の安定性*: 指数的な減衰により、長期予測の誤差が過度に大きくなることを防ぎ、学習の安定性を確保します。

4. *ホライズンに応じた適応*: $gamma = e^(-1/D)$ の設定により、ホライズン長 $D$ に合わせた自然な重み付けが可能になります。
